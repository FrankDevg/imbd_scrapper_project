# ğŸ¬ IMDb Scraper â€“ Arquitectura Limpia para Scraping Distribuido y Persistencia HÃ­brida

Este proyecto es la soluciÃ³n integral a la prueba tÃ©cnica, demostrando la capacidad de construir un sistema de extracciÃ³n de datos robusto, escalable y mantenible. Se ha desarrollado un scraper para el Top 250 de IMDb, implementando tÃ©cnicas avanzadas de evasiÃ³n de bloqueos, persistencia en PostgreSQL y una arquitectura de software desacoplada, lista para evolucionar.

El diseÃ±o se fundamenta en principios de **Clean Architecture** y **Domain-Driven Design (DDD)**, estÃ¡ completamente **orquestado con Docker**, y documenta una estrategia clara para escalar hacia herramientas como Playwright o Selenium si las defensas del sitio objetivo lo requiriesen.

---
## ğŸ§­ Tabla de Contenido

- [âœ… Objetivos Cumplidos y Cobertura de Requisitos](#-objetivos-cumplidos-y-cobertura-de-requisitos)
- [ğŸ›ï¸ FilosofÃ­a de Arquitectura y Decisiones TÃ©cnicas](#ï¸-filosofÃ­a-de-arquitectura-y-decisiones-tÃ©cnicas)
- [ğŸ§± Estructura del Proyecto](#-estructura-del-proyecto)
- [ğŸ§© Estrategia de Red Distribuida: VPN + Proxies + TOR](#-estrategia-de-red-distribuida-vpn--proxies--tor)
- [ğŸ³ Instrucciones de Despliegue con Docker](#-instrucciones-de-despliegue-con-docker)
- [ğŸ§  SQL AnalÃ­tico](#-sql-analÃ­tico)
- [4ï¸âƒ£ ComparaciÃ³n y Escalabilidad: Scrapy vs. Playwright/Selenium](#4ï¸âƒ£-comparaciÃ³n-y-escalabilidad-scrapy-vs-playwrightselenium)
- [ğŸ§µ Concurrencia Aplicada en el Scraper](#-concurrencia-aplicada-en-el-scraper)
- [ğŸ” Decisiones TÃ©cnicas Clave](#-decisiones-tÃ©cnicas-clave)
- [ğŸ“¦ Entregables Finales](#-entregables-finales)
- [ğŸ“£ CrÃ©ditos](#-crÃ©ditos)
---
## âœ… Objetivos Cumplidos y Cobertura de Requisitos

Se ha cumplido con el 100% de los requisitos solicitados, tanto obligatorios como opcionales, para demostrar una competencia exhaustiva en cada Ã¡rea evaluada.

| Ãrea de EvaluaciÃ³n | âœ… Implementado | Detalle TÃ©cnico de la ImplementaciÃ³n |
| :--- | :---: | :--- |
| **Scraping (60%)** | âœ… SÃ­ | Se extraen las 250 pelÃ­culas del chart, obteniendo TÃ­tulo, AÃ±o, CalificaciÃ³n, DuraciÃ³n, Metascore y Actores desde las pÃ¡ginas de detalle. La soluciÃ³n es modular y maneja errores con reintentos y backoff exponencial. |
| **Arquitectura Avanzada** | âœ… SÃ­ | Se implementÃ³ **Clean Architecture + DDD** y el patrÃ³n **Factory** para desacoplar la lÃ³gica de negocio de la infraestructura (ej. persistencia, scraping). |
| **Persistencia CSV + SQL** | âœ… SÃ­ | Los datos se persisten de forma hÃ­brida en archivos CSV y en un esquema relacional PostgreSQL con una relaciÃ³n `N:M` entre pelÃ­culas y actores. |
| **SQL AnalÃ­tico (20%)** | âœ… SÃ­ | Se crearon consultas analÃ­ticas complejas utilizando **funciones de ventana (`OVER/PARTITION BY`)**, vistas, Ã­ndices y justificaciÃ³n de particionamiento. |
| **Proxies y Red (10%)** | âœ… SÃ­ | Se implementÃ³ una arquitectura distribuida con mÃºltiples capas de evasiÃ³n: conexiÃ³n mediante **VPN real (ProtonVPN vÃ­a Docker)**, uso de **proxies premium (DataImpulse)** y fallback automÃ¡tico a la **red TOR**. AdemÃ¡s, se incluyÃ³ un sistema de **reintentos con backoff exponencial** y validaciÃ³n de IP para garantizar la obtenciÃ³n del dato. |
| **Dockerizado y Portable** | âœ… SÃ­ | Todo el entorno (scraper, base de datos) se levanta con un solo comando (`docker-compose up`), garantizando la replicabilidad del entorno. |
| **ComparaciÃ³n Herramientas (10%)** | âœ… SÃ­ | Se incluye una secciÃ³n detallada justificando cuÃ¡ndo y cÃ³mo migrar a **Playwright/Selenium** para escenarios de JavaScript dinÃ¡mico y CAPTCHAs. |
| **Evidencia y DocumentaciÃ³n** | âœ… SÃ­ | El repositorio incluye logs, scripts SQL, CSVs generados y este README detallado como evidencia del trabajo realizado. |

---

## ğŸ›ï¸ FilosofÃ­a de Arquitectura y Decisiones TÃ©cnicas

### Â¿Por quÃ© Clean Architecture + Domain-Driven Design (DDD)?
Un enfoque profesional exige construir un **sistema mantenible y escalable**.

- **SeparaciÃ³n de Responsabilidades (SoC):** Las dependencias apuntan hacia adentro. La lÃ³gica de negocio no sabe nada sobre la base de datos ni el scraping.
- **Testabilidad Aislada:** Las capas `domain` y `application` se pueden testear unitariamente sin dependencias externas.
- **Modelado del Dominio:** Entidades como `Movie` y `Actor` reflejan el lenguaje del problema, con validaciones integradas.

### Â¿Por quÃ© el PatrÃ³n Factory?
Se utiliza para desacoplar la lÃ³gica de negocio de las implementaciones concretas.

- Permite cambiar la fuente de persistencia (CSV, PostgreSQL, MongoDB) sin modificar la lÃ³gica del caso de uso.
- Cumple con el Principio Abierto/Cerrado.

### Â¿Por quÃ© TOR para la RotaciÃ³n de IPs?
TOR ofrece:

- **RotaciÃ³n efectiva de IPs** sin costo adicional.
- **Independencia del proveedor:** Se puede sustituir fÃ¡cilmente por otros proxies comerciales o VPNs.

---

## ğŸ§± Estructura del Proyecto

```
imbd_scraper_project/
â”œâ”€â”€ application/                  # Casos de uso orquestando lÃ³gica de dominio
â”‚   â””â”€â”€ use_cases/
â”‚       â”œâ”€â”€ composite_save_movie_with_actors_use_case.py
â”‚       â”œâ”€â”€ save_movie_with_actors_csv_use_case.py
â”‚       â”œâ”€â”€ save_movie_with_actors_postgres_use_case.py
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ data/                         # Archivos CSV generados automÃ¡ticamente
â”‚   â”œâ”€â”€ actors.csv
â”‚   â”œâ”€â”€ movies.csv
â”‚   â””â”€â”€ movie_actor.csv
â”‚
â”œâ”€â”€ domain/                       # Modelos, interfaces y contratos de repositorio
â”‚   â”œâ”€â”€ interfaces/               # Interfaces de scraper, proxy, etc.
â”‚   â”‚   â”œâ”€â”€ proxy_interface.py
â”‚   â”‚   â”œâ”€â”€ scraper_interface.py
â”‚   â”‚   â”œâ”€â”€ tor_interface.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/                   # Entidades del dominio
â”‚   â”‚   â”œâ”€â”€ actor.py
â”‚   â”‚   â”œâ”€â”€ movie.py
â”‚   â”‚   â”œâ”€â”€ movie_actor.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ repositories/            # Contratos de repositorios
â”‚       â”œâ”€â”€ actor_repository.py
â”‚       â”œâ”€â”€ movie_actor_repository.py
â”‚       â”œâ”€â”€ movie_repository.py
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ infrastructure/              # Implementaciones tecnolÃ³gicas
â”‚   â”œâ”€â”€ factory/                 # Factories para desacoplar la creaciÃ³n de objetos
â”‚   â”‚   â”œâ”€â”€ db_factory.py
â”‚   â”‚   â”œâ”€â”€ proxy_factory.py
â”‚   â”‚   â”œâ”€â”€ scraper_factory.py
â”‚   â”‚   â”œâ”€â”€ tor_factory.py
â”‚   â”‚   â”œâ”€â”€ use_case_factory.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ persistence/             # Implementaciones concretas de persistencia
â”‚   â”‚   â”œâ”€â”€ csv/
â”‚   â”‚   â”‚   â”œâ”€â”€ init_csv_files.py
â”‚   â”‚   â”‚   â””â”€â”€ repositories/
â”‚   â”‚   â”‚       â”œâ”€â”€ actor_csv_repository.py
â”‚   â”‚   â”‚       â”œâ”€â”€ movie_actor_csv_repository.py
â”‚   â”‚   â”‚       â””â”€â”€ movie_csv_repository.py
â”‚   â”‚   â””â”€â”€ postgres/
â”‚   â”‚       â”œâ”€â”€ postgres_connection.py
â”‚   â”‚       â””â”€â”€ repositories/
â”‚   â”‚           â”œâ”€â”€ actor_postgres_repository.py
â”‚   â”‚           â”œâ”€â”€ movie_actor_postgres_repository.py
â”‚   â”‚           â””â”€â”€ movie_postgres_repository.py
â”‚   â”œâ”€â”€ proxy_rotation/          # LÃ³gica de rotaciÃ³n TOR
â”‚   â”‚   â””â”€â”€ tor_rotator.py
â”‚   â”œâ”€â”€ provider/                # LÃ³gica de selecciÃ³n de proxy (ej. TOR, otros)
â”‚   â”‚   â””â”€â”€ proxy_provider.py
â”‚   â””â”€â”€ scraper/                 # ImplementaciÃ³n del scraper principal
â”‚       â”œâ”€â”€ imdb_scraper.py
â”‚       â”œâ”€â”€ utils.py
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ logs/                        # Archivos de logs del scraper
â”‚   â””â”€â”€ scraper.log
â”‚
â”œâ”€â”€ presentation/                # CLI o interfaces externas
â”‚   â””â”€â”€ cli/
â”‚       â”œâ”€â”€ run_scraper.py       # Punto de entrada principal
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ shared/                      # Utilidades globales
â”‚   â”œâ”€â”€ config/                  # Configuraciones centralizadas
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â””â”€â”€ logger/                  # ConfiguraciÃ³n de logging
â”‚       â””â”€â”€ logging_config.py
â”‚
â”œâ”€â”€ sql/                         # Scripts SQL para DB
â”‚   â”œâ”€â”€ 01_schema.sql
â”‚   â”œâ”€â”€ 02_procedures.sql
â”‚   â”œâ”€â”€ 03_views.sql
â”‚   â”œâ”€â”€ load_from_csv.sql
â”‚   â””â”€â”€ queries.sql
â”‚
â”œâ”€â”€ .env                         # Configuraciones de entorno (no versionar)
â”œâ”€â”€ .gitignore                   # Ignora .env, __pycache__, etc.
â”œâ”€â”€ docker-compose.yml           # OrquestaciÃ³n del proyecto con PostgreSQL
â”œâ”€â”€ Dockerfile                   # Imagen del scraper
â”œâ”€â”€ requirements.txt             # Dependencias del proyecto
â””â”€â”€ README.md                    # DocumentaciÃ³n completa del sistema
```
---

### ğŸ§© Estrategia de Red Distribuida: VPN + Proxies + TOR

El scraper estÃ¡ preparado para ejecutar en ambientes con **alta sensibilidad al bloqueo**, usando una combinaciÃ³n de estrategias en capas para garantizar la recolecciÃ³n de datos:

| TecnologÃ­a        | PropÃ³sito                                    | ImplementaciÃ³n                                                      |
|------------------|----------------------------------------------|----------------------------------------------------------------------|
| **VPN (ProtonVPN)**     | Cambiar geolocalizaciÃ³n y evitar bloqueo regional | Montada en **Docker**, validaciÃ³n de paÃ­s vÃ­a healthcheck por imagen qmcgaw/gluetun            |
| **Proxies Premium**     | IPs rotativas anÃ³nimas, baja latencia            | IntegraciÃ³n con **DataImpulse**, rotaciÃ³n automÃ¡tica por cada request |
| **TOR (Fallback)**      | Red distribuida anÃ³nima gratuita                 | ActivaciÃ³n automÃ¡tica en caso de fallo en las otras capas            |

AdemÃ¡s, se integrÃ³ un **sistema de reintentos inteligentes con backoff exponencial** que asegura que la peticiÃ³n se repita en caso de fallo, cambiando IP si es necesario, y dejando trazabilidad en logs con la IP usada.

---

### ğŸ”§ Posibles Mejoras Futuras

- **Proxy Pool DinÃ¡mico** con rotaciÃ³n basada en reputaciÃ³n/IP-bans.
- **Auto-restart de TOR/VPN** si el healthcheck falla.
- IntegraciÃ³n con **servicios anti-CAPTCHA** como 2Captcha o Anti-Captcha.
- Compatibilidad con **geolocalizaciÃ³n dinÃ¡mica por paÃ­s**, seleccionando el proxy o VPN mÃ¡s adecuado segÃºn el sitio.

---

## ğŸ³ Instrucciones de Despliegue con Docker

1. Crear el archivo `.env` en la carpeta raiz  
### ğŸ“„ Archivo `.env` (proporcionado solo con fines de evaluaciÃ³n)

Este archivo contiene la configuraciÃ³n necesaria para conectar con los servicios de base de datos, proxies y VPN utilizados en el entorno del scraper.

**âš ï¸ Importante:** Las credenciales contenidas en este archivo son simuladas y han sido incluidas Ãºnicamente para facilitar la evaluaciÃ³n del proyecto. En un entorno real, se recomienda gestionar estas variables de forma segura mediante servicios como Docker Secrets, AWS Parameter Store o `.env` ignorado por `.gitignore`.

Ejemplo de `.env`:

```env

POSTGRES_DB=imdb_scraper
POSTGRES_USER=aruiz
POSTGRES_PASSWORD=@ndresruiz@123
POSTGRES_PORT=5432
POSTGRES_HOST=postgres


PROXY_HOST=gw.dataimpulse.com
PROXY_PORT=823
PROXY_USER=f1bdc8e207aafe131216
PROXY_PASS=6c1b9cdd85f65f0b

VPN_PROVIDER=protonvpn
VPN_USERNAME=qlT5gZGnlHi2Y1uh
VPN_PASSWORD=mUiXGzoM9SeYHhYocElsEMuQwUUGuLFL
VPN_COUNTRY=Argentina

```
2. Ejecutar:
   
```bash
docker-compose build --no-cache
```

```bash
docker-compose up
```

- PostgreSQL expuesto en `localhost:5432`.
- Scraper inicia automÃ¡ticamente.
- Logs en `logs/scraper.log`.
- Archivos `movies.csv`, `actors.csv`, `movie_actor.csv` generados en `/data`.

---

## ğŸ§  SQL AnalÃ­tico

Incluye:

- ğŸï¸ **Top 5 por duraciÃ³n promedio por dÃ©cada** â€“ con `ROW_NUMBER()` y `PARTITION BY`.
- ğŸ“ˆ **DesviaciÃ³n estÃ¡ndar de calificaciÃ³n por aÃ±o** â€“ mide dispersiÃ³n de opiniones.
- âš–ï¸ **ComparaciÃ³n IMDb vs Metascore** â€“ normalizado y filtrado por delta > 20%.
- ğŸ‘¥ **Vista Actor-PelÃ­cula** â€“ facilita filtrado y joins.
- âš¡ **Ãndices y vistas materializadas** â€“ optimizaciÃ³n de rendimiento.

---

## 4ï¸âƒ£ ComparaciÃ³n y Escalabilidad: Scrapy vs. Playwright/Selenium

Aunque este proyecto estÃ¡ construido con `requests` y `BeautifulSoup` por su requerimiento y control detallado del flujo, estÃ¡ preparado para escalar hacia herramientas como **Playwright** o **Selenium** en los siguientes escenarios:

### ğŸ”§ ConfiguraciÃ³n avanzada de navegador
Ambas herramientas permiten lanzar navegadores con configuraciones avanzadas:

- **Modo headless o visible** (`headless=True/False`).
- **ModificaciÃ³n de headers personalizados** como User-Agent, Referer, Accept-Language, etc.
- **EvasiÃ³n de detecciÃ³n WebDriver**:
  - Redefinir `navigator.webdriver`.
  - Inyectar scripts personalizados en el contexto de la pÃ¡gina.
  - Usar extensiones anti-bot o librerÃ­as como `stealth.min.js` en Playwright.

### ğŸ¯ Selectores dinÃ¡micos con espera explÃ­cita
- **Playwright**: `page.wait_for_selector("selector")` asegura que el DOM estÃ© listo.
- **Selenium**: `WebDriverWait(driver, timeout).until(expected_conditions.presence_of_element_located(...))` permite esperar elementos dinÃ¡micos cargados vÃ­a JavaScript.

Esto evita fallos comunes en scraping dinÃ¡mico (ej: `element not found` o `NoneType`).

### ğŸ›¡ï¸ Manejo de JavaScript rendering y CAPTCHAs
- **Renderizado completo del DOM** habilitado por defecto al usar navegadores reales.
- **CAPTCHAs**:
  - Detectar presencia de CAPTCHA mediante selectores.
  - Resolverlo usando APIs de servicios como **2Captcha**, **AntiCaptcha**, **DeathByCaptcha**.
  - Alternativamente, utilizar OCR bÃ¡sico si el CAPTCHA es visualmente simple.

### âš™ï¸ Control de concurrencia
- **Playwright**:
  - Permite abrir mÃºltiples contextos (`browser.new_context()`) o mÃºltiples pÃ¡ginas en paralelo.
  - Ideal para scraping distribuido sin overhead de mÃºltiples procesos.

- **Selenium**:
  - Compatible con **Selenium Grid** para distribuir instancias en mÃºltiples nodos.
  - Puede ejecutarse en contenedores paralelos coordinados mediante colas (ej. **Celery**, **RabbitMQ**).

- **Ambas** pueden integrarse en workers asÃ­ncronos si se envuelven correctamente.

### ğŸ“Œ JustificaciÃ³n vs Scrapy
Aunque **Scrapy** es potente y extensible, **Playwright** y **Selenium** ofrecen ventajas cuando:
- El contenido depende de **JavaScript o eventos del navegador**.
- Se requiere **simular interacciÃ³n humana real**: scroll, clics, selecciÃ³n dinÃ¡mica.
- El sitio tiene **bloqueos activos** como CAPTCHAs, honeypots o detecciÃ³n de trÃ¡fico automatizado.

## ğŸ§© ImplementaciÃ³n con Playwright o Selenium

Gracias a la aplicaciÃ³n de Clean Architecture y DDD, el sistema permite **agregar nuevos engines de scraping** (como Playwright o Selenium) sin reemplazar ni modificar la implementaciÃ³n actual basada en `requests + BeautifulSoup`.

### ğŸ”„ Â¿CÃ³mo se logra esto?

La clave estÃ¡ en el uso de interfaces y fÃ¡bricas desacopladas:

- `ScraperInterface` en `domain/` define el contrato Ãºnico que todas las implementaciones deben seguir.
- Cada implementaciÃ³n (ej. `ImdbScraperRequests`, `ImdbScraperPlaywright`) vive en su propio archivo dentro de `infrastructure/scraper/`.
- Una fÃ¡brica central (`get_scraper()`) puede decidir quÃ© engine usar.

### âš™ï¸ Alternativa para elegir el engine

Se puede agregar una variable de entorno en `config.py` para permitir elegir dinÃ¡micamente el motor de scraping:

```python
# shared/config/config.py
SCRAPER_ENGINE = "Playwright"
```

```python
# infrastructure/scraper/factory.py
from infrastructure.scraper.imdb_scraper_requests import ImdbScraperRequests
from infrastructure.scraper.imdb_scraper_playwright import ImdbScraperPlaywright
from shared.config import config

def get_scraper(source: str, engine: str = "requests") -> ScraperInterface:
    source_clean = source.strip().lower()
    engine_clean = engine.strip().lower()

    if source_clean == "imdb":
        if engine_clean == "requests":
            from infrastructure.scraper.imdb_scraper import ImdbScraper
            return ImdbScraper()
        elif engine_clean == "playwright":
            from infrastructure.scraper.imdb_scraper_playwright import ImdbScraperPlaywright
            return ImdbScraperPlaywright()

    raise ValueError(f"Scraper no soportado: source='{source}', engine='{engine}'")

```

### ğŸ” Â¿Y si quiero usar ambos al mismo tiempo?

TambiÃ©n es posible. Puedes inyectar ambos scrapers en un **composite scraper** que combine o compare resultados, o usarlos como fallback uno del otro:

```python
class CompositeScraper(ScraperInterface):
    def __init__(self, primary_scraper, secondary_scraper):
        self.primary = primary_scraper
        self.secondary = secondary_scraper

    def scrape(self):
        try:
            return self.primary.scrape()
        except Exception:
            return self.secondary.scrape()
```

> Esto aporta escalabilidad sin comprometer el diseÃ±o, permitiendo usar mÃºltiples motores sin reescribir los casos de uso.


---

## ğŸ§µ Concurrencia Aplicada en el Scraper

El scraper utiliza **`ThreadPoolExecutor`** desde la librerÃ­a `concurrent.futures` para acelerar la recolecciÃ³n de informaciÃ³n de detalle por pelÃ­cula.

### ğŸ§  Detalles tÃ©cnicos:
- Se limita el nÃºmero de threads para evitar saturar la red o el endpoint.
- Cada hilo ejecuta la funciÃ³n de extracciÃ³n del detalle de la pelÃ­cula (`/title/{id}/`) en paralelo.
- Esto mejora el rendimiento sin comprometer la trazabilidad ni la estructura del log.

Se podrÃ­an reemplazar por workers distribuidos en producciÃ³n para escalar horizontalmente.

---

## ğŸ” Decisiones TÃ©cnicas Clave

### ğŸ§  1. SQL Directo en lugar de ORM
DecidÃ­ **no utilizar un ORM como SQLAlchemy** y optar por sentencias SQL explÃ­citas, basÃ¡ndome en:

- âœ… **Simplicidad del modelo de datos** (pelÃ­culas, actores y relaciÃ³n N:M).
- âœ… **Mayor control sobre las operaciones** de escritura, validaciones y consultas analÃ­ticas.
- âœ… **SeparaciÃ³n limpia por repositorios**, que permite desacoplar la lÃ³gica de persistencia y facilitar una futura migraciÃ³n a un ORM sin modificar los casos de uso.
- âœ… **Mejor rendimiento para scraping masivo**, al evitar capas adicionales de abstracciÃ³n.

> Esta decisiÃ³n no limita la escalabilidad futura, ya que el diseÃ±o permite incorporar ORM cuando sea necesario.

---

### ğŸŒ 2. Scraping distribuido con rotaciÃ³n de IPs y red privada

Para garantizar robustez y anonimato en la extracciÃ³n de datos, el scraper estÃ¡ configurado con:

- ğŸ§… **Red TOR** activa para rotaciÃ³n bÃ¡sica de IPs.
- ğŸ”„ **User-Agent aleatorios y headers realistas** en cada solicitud.
- ğŸ§° **Proxies premium de Data Impulso**, integrados con fallback automÃ¡tico.
- ğŸ” **VPN real instalada dentro de Docker**, conectada a la red interna.
- ğŸ’£ Tolerancia a fallos mediante reintentos automÃ¡ticos y separaciÃ³n del canal de scraping y persistencia.

---

### ğŸ—‚ï¸ 3. Persistencia hÃ­brida (CSV + PostgreSQL)

Para garantizar versatilidad en el anÃ¡lisis y almacenamiento:

- ğŸ§¾ **CSV**: ExportaciÃ³n directa a `movies.csv`, `actors.csv` y `movie_actor.csv`, Ãºtil para revisiÃ³n rÃ¡pida o carga en herramientas externas.
- ğŸ›¢ï¸ **PostgreSQL**: Almacenamiento estructurado de pelÃ­culas, actores y relaciones, ideal para anÃ¡lisis SQL avanzado y consultas cruzadas.
- ğŸ§± Cada mecanismo de persistencia se implementÃ³ como un repositorio independiente bajo el patrÃ³n Strategy, permitiendo su uso simultÃ¡neo o alternativo.

---

### ğŸ§¼ 4. Arquitectura Limpia (Clean Architecture + DDD)

Todo el proyecto fue estructurado en capas bien definidas:

- `domain/`: Modelos de negocio y contratos de repositorios (interfaces).
- `application/`: Casos de uso desacoplados.
- `infrastructure/`: Implementaciones concretas (scraper, CSV, DB).
- `presentation/`: Punto de entrada (`run_scraper.py`).
- `shared/`: ConfiguraciÃ³n, logging y constantes.

> Esta estructura facilita pruebas unitarias, extensibilidad y mantenimiento a largo plazo.

---

### ğŸ‹ 5. Entorno Dockerizado con Red Privada Segura

El entorno de ejecuciÃ³n estÃ¡ totalmente containerizado y preparado para producciÃ³n:

- `docker-compose.yml` levanta servicios clave:
  - Scraper
  - PostgreSQL
  - Red TOR
  - **VPN montada en la red interna**
- Se utilizaron:
  - âœ… **Redes internas de Docker**
  - âœ… **VolÃºmenes persistentes**
  - âœ… **Variables externas (.env)**
- Se integraron **healthchecks propios de cada imagen Docker** para garantizar estabilidad de los servicios antes de ejecutar el scraping.
---
## ğŸ“¦ Entregables Finales

- ğŸ—ƒï¸ CÃ³digo en GitHub
- ğŸ˜ Scripts SQL en `/sql/`
- ğŸ“„ CSVs generados en `/data/`
- ğŸ§¾ Logs con rotaciÃ³n IP en `/logs/`
- ğŸ“˜ DocumentaciÃ³n tÃ©cnica (este archivo) y Documento de arquitectura `/docs/`

---

## ğŸ“£ CrÃ©ditos

Proyecto desarrollado por **AndrÃ©s Ruiz** para la prueba tÃ©cnica de Scraping Senior.  
ğŸ“« Email: franklindbruiz@gmail.com  
ğŸ”— GitHub: [frankdevg](https://github.com/frankdevg)
